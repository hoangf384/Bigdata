import json
import glob
import os
import time
import asyncio
from dotenv import load_dotenv
from openai import OpenAI, AsyncOpenAI
import polars as pl

# API workers (N)  ‚îÄ‚îÄ‚ñ∫  Queue  ‚îÄ‚îÄ‚ñ∫  Writer (1)  ‚îÄ‚îÄ‚ñ∫  mapping.jsonl


# ---------------------------
# init
# ---------------------------

load_dotenv()
client = OpenAI(api_key = os.getenv("OPENAI_API_KEY"))
async_client = AsyncOpenAI(api_key=os.getenv("OPENAI_API_KEY"))

# ---------------------------
# path
# ---------------------------

folder_path = "./Data/raw/log_search/"
save_path = "./Data/destination/log_search/category/"


# ---------------------------
# IO helpers
# ---------------------------


def read_data(data_type: str, path: str) -> pl.DataFrame:
    """
    ƒë·ªçc d·ªØ li·ªáu v√† tr·∫£ v·ªÅ dataframe

    Parameters
    ----------
    data_type : str
        file extension h·ªó tr·ª£ l√† "parquet", "jsonl".
    path : str
        ƒë∆∞·ªùng d·∫´n tr·ª±c ti·∫øp (ƒë·ªëi v·ªõi parquet) ho·∫∑c ƒë∆∞·ªùng d·∫´n d√°n ti·∫øp (t√™n file ƒë·ªëi v·ªõi jsonl) 

    Returns
    -------
    polars.DataFrame

    Raises
    ------
    ValueError
        If input files are missing or data_type is unsupported.
    """

    if data_type == "parquet":
        files = glob.glob(os.path.join(path, "**/*.parquet"), recursive=True)

        if not files:
            raise ValueError(f"No parquet files found under path: {path}")

        try:
            data = pl.read_parquet(files)
            return data
        except Exception as e:
            raise RuntimeError(f"Failed to read parquet files: {e}") from e


    elif data_type == "jsonl":
        

        if not os.path.exists(path):
            raise ValueError(f"JSONL file not found: {path}")

        if os.path.getsize(path) == 0:
            
            return pl.DataFrame(
                {
                    "keyword": pl.Series([], dtype=pl.Utf8),
                    "category": pl.Series([], dtype=pl.Utf8),
                }
            )
        return pl.read_ndjson(path)
    
    else:
        raise ValueError(f"Unsupported data_type: {data_type}")




def save_data(data, path):
    """
    Docstring for save_data
    
    :param data: Description
    :param path: Description
    """
    # h·ªèi chatGPT ƒë·ªÉ bi·∫øt ƒë∆∞·ª£c () c·∫ßn tham s·ªë g√¨
    data.write_parquet(path)



def clean_llm_json(text: str) -> str:
    """
    L√†m s·∫°ch output JSON t·ª´ LLM, lo·∫°i b·ªè markdown n·∫øu c√≤n s√≥t.

    Parameters
    ----------
    text : str
        Chu·ªói output t·ª´ LLM.

    Returns
    -------
    str
        Chu·ªói JSON s·∫°ch, s·∫µn s√†ng ƒë·ªÉ parse.
    """
    text = text.strip()

    if text.startswith("```"):
        text = text.replace("```json", "").replace("```", "").strip()

    return text



def read_jsonl(path):
    rows = []
    with open(path, "r", encoding="utf-8") as f:
        for line in f:
            try:
                rows.append(json.loads(line))
            except json.JSONDecodeError:
                pass
    return pl.DataFrame(rows)


# ---------------------------
# transform
# ---------------------------


def get_data(data: pl.DataFrame) -> list[str]:
    """
    Tr√≠ch xu·∫•t danh s√°ch keyword duy nh·∫•t t·ª´ DataFrame ƒë·∫ßu v√†o.

    Parameters
    ----------
    data : polars.DataFrame
        DataFrame ch·ª©a d·ªØ li·ªáu log, b·∫Øt bu·ªôc c√≥ c·ªôt `keyword`.

    Returns
    -------
    list of str
        Danh s√°ch keyword (ki·ªÉu Python list), ƒë√£:
        - lo·∫°i b·ªè gi√° tr·ªã null
        - lo·∫°i b·ªè tr√πng l·∫∑p
    """
    keywords = (
        data.select("keyword")
            .drop_nulls()
            .unique()
            .to_series()
            .to_list()
    )
    return keywords

#
#
#


def init_output_folder(path: str) -> None:
    """
    Kh·ªüi t·∫°o th∆∞ m·ª•c output v√† file mapping n·∫øu ch∆∞a t·ªìn t·∫°i.
    """
    if os.path.exists(path) and not os.path.isdir(path):
        raise ValueError(f"Output path exists but is not a directory: {path}")

    os.makedirs(path, exist_ok=True)

    mapping_file = os.path.join(path, "mapping.jsonl")
    if not os.path.exists(mapping_file):
        with open(mapping_file, "w", encoding="utf-8"):
            pass



def chunks(lst: list, size: int):
    """
    Chia m·ªôt danh s√°ch th√†nh c√°c batch con c√≥ k√≠ch th∆∞·ªõc c·ªë ƒë·ªãnh.

    Parameters
    ----------
    lst : list
        Danh s√°ch ƒë·∫ßu v√†o c·∫ßn ƒë∆∞·ª£c chia nh·ªè.
    size : int
        K√≠ch th∆∞·ªõc t·ªëi ƒëa c·ªßa m·ªói batch.

    Yields
    ------
    list
        C√°c danh s√°ch con (batch) c√≥ ƒë·ªô d√†i kh√¥ng v∆∞·ª£t qu√° `size`.

    Notes
    -----
    - Batch cu·ªëi c√πng c√≥ th·ªÉ c√≥ s·ªë ph·∫ßn t·ª≠ nh·ªè h∆°n `size`.
    - H√†m n√†y th∆∞·ªùng ƒë∆∞·ª£c d√πng ƒë·ªÉ:
        * chia d·ªØ li·ªáu x·ª≠ l√Ω theo l√¥ (batch processing)
        * gi·ªõi h·∫°n s·ªë ph·∫ßn t·ª≠ khi g·ªçi API ho·∫∑c LLM
        * tr√°nh v∆∞·ª£t qu√° memory ho·∫∑c rate limit
    """
    for i in range(0, len(lst), size):
        yield lst[i:i + size]


# ----------------------------
# for function
# ----------------------------

def build_prompt(movie_list):
    return f"""B·∫°n l√† m·ªôt chuy√™n gia ph√¢n lo·∫°i n·ªôi dung phim, ch∆∞∆°ng tr√¨nh truy·ªÅn h√¨nh v√† n·ªôi dung gi·∫£i tr√≠ t·∫°i Vi·ªát Nam.

B·∫°n s·∫Ω nh·∫≠n m·ªôt danh s√°ch keyword t√¨m ki·∫øm, c√≥ th·ªÉ:
- vi·∫øt sai ch√≠nh t·∫£
- kh√¥ng d·∫•u
- vi·∫øt li·ªÅn
- vi·∫øt t·∫Øt
- ho·∫∑c ch·ªâ l√† c·ª•m t·ª´ g·ª£i √Ω m∆° h·ªì

‚ö†Ô∏è NGUY√äN T·∫ÆC B·∫ÆT BU·ªòC (C·ª∞C K·ª≤ QUAN TR·ªåNG):
- M·ªñI keyword PH·∫¢I ƒë∆∞·ª£c g√°n CH√çNH X√ÅC 1 th·ªÉ lo·∫°i.
- TUY·ªÜT ƒê·ªêI KH√îNG tr·∫£ v·ªÅ "Other" n·∫øu c√≤n b·∫•t k·ª≥ c√°ch suy ƒëo√°n h·ª£p l√Ω n√†o.
- "Other" CH·ªà ƒë∆∞·ª£c d√πng khi keyword ho√†n to√†n v√¥ nghƒ©a, spam, ho·∫∑c kh√¥ng li√™n quan n·ªôi dung gi·∫£i tr√≠.

NHI·ªÜM V·ª§:
1. Chu·∫©n ho√° v√† s·ª≠a l·ªói keyword (CH·ªà ƒë·ªÉ suy lu·∫≠n n·ªôi b·ªô, KH√îNG ghi ra output).
2. Nh·∫≠n di·ªán √Ω nghƒ©a g·ªëc g·∫ßn ƒë√∫ng nh·∫•t (phim, show, b√†i h√°t, s·ª± ki·ªán, ƒë·ªôi tuy·ªÉn, m√¥ t·∫£ n·ªôi dung).
3. G√°n th·ªÉ lo·∫°i PH√ô H·ª¢P NH·∫§T trong danh s√°ch d∆∞·ªõi ƒë√¢y.

DANH S√ÅCH TH·ªÇ LO·∫†I H·ª¢P L·ªÜ (CH·ªà ƒê∆Ø·ª¢C CH·ªåN 1):
- Action
- Romance
- Comedy
- Horror
- Animation
- Drama
- C Drama
- K Drama
- Sports
- Music
- Reality Show
- TV Channel
- News
- Other

LU·∫¨T SUY DI·ªÑN ∆ØU TI√äN (PH·∫¢I TU√ÇN THEO):
- C√≥ "t·∫≠p", "episode", "ep":
    ‚Ä¢ N·∫øu keyword ch·ª©a t√™n show / gameshow / reality quen thu·ªôc
      (v√≠ d·ª•: running man, 2 ng√†y 1 ƒë√™m, rap vi·ªát, the voice, masterchef)
      ‚Üí Reality Show
    ‚Ä¢ N·∫øu ch·ª©a t·ª´ kho√° phim / series / h√†nh ƒë·ªông
      ‚Üí Drama ho·∫∑c Action
- Karaoke, b√†i h√°t, ca sƒ©, l·ªùi b√†i h√°t, remix ‚Üí Music
- Tr·∫≠n ƒë·∫•u, b√≥ng ƒë√°, ƒë·ªôi tuy·ªÉn, U19, Vi·ªát Nam vs ‚Üí Sports
- T√™n phim, series, ti√™u ƒë·ªÅ truy·ªán (k·ªÉ c·∫£ m∆° h·ªì, vi·∫øt sai) ‚Üí Drama
- Phim Trung Qu·ªëc ‚Üí C Drama
- Phim H√†n Qu·ªëc ‚Üí K Drama
- Show truy·ªÅn h√¨nh, gameshow ‚Üí Reality Show
- T√™n k√™nh (VTV, HTV, K+, HBO, Channel) ‚Üí TV Channel
- Keyword ng·∫Øn gi·ªëng t√™n ri√™ng / ti√™u ƒë·ªÅ ‚Üí ∆∞u ti√™n Drama ho·∫∑c Music
- Ch·ªâ d√πng Other khi keyword kh√¥ng th·ªÉ g√°n v√†o b·∫•t k·ª≥ nh√≥m n√†o ·ªü tr√™n

OUTPUT:
- Ch·ªâ tr·∫£ v·ªÅ 1 JSON object
- Key = keyword g·ªëc trong danh s√°ch (KH√îNG s·ª≠a)
- Value = th·ªÉ lo·∫°i ƒë√£ ph√¢n lo·∫°i
- KH√îNG gi·∫£i th√≠ch, KH√îNG th√™m text ngo√†i JSON

Danh s√°ch keyword:
{movie_list}
"""

async def classify_batch_async(movie_list):
    if not movie_list:
        return {}

    prompt = build_prompt(movie_list)

    try:
        resp = await async_client.chat.completions.create(
            model="gpt-5-nano",
            messages=[{"role": "user", "content": prompt}],
            response_format={"type": "json_object"}
        )

        text = resp.choices[0].message.content
        text = clean_llm_json(text)

        parsed = json.loads(text)

        # üîí ƒë·∫£m b·∫£o kh√¥ng thi·∫øu key
        return {k: parsed.get(k, "Other") for k in movie_list}

    except Exception as e:
        print("LLM JSON parse error:", e)
        return {k: "Other" for k in movie_list}


# def classify_batch(movie_list):
#     """
#     Docstring for classify_batch
    
#     :param movie_list: Description
#     """
#     if not movie_list:
#         return {}
    
#     prompt = f"""B·∫°n l√† m·ªôt chuy√™n gia ph√¢n lo·∫°i n·ªôi dung phim, ch∆∞∆°ng tr√¨nh truy·ªÅn h√¨nh v√† n·ªôi dung gi·∫£i tr√≠ t·∫°i Vi·ªát Nam.

# B·∫°n s·∫Ω nh·∫≠n m·ªôt danh s√°ch keyword t√¨m ki·∫øm, c√≥ th·ªÉ:
# - vi·∫øt sai ch√≠nh t·∫£
# - kh√¥ng d·∫•u
# - vi·∫øt li·ªÅn
# - vi·∫øt t·∫Øt
# - ho·∫∑c ch·ªâ l√† c·ª•m t·ª´ g·ª£i √Ω m∆° h·ªì

# ‚ö†Ô∏è NGUY√äN T·∫ÆC B·∫ÆT BU·ªòC (C·ª∞C K·ª≤ QUAN TR·ªåNG):
# - M·ªñI keyword PH·∫¢I ƒë∆∞·ª£c g√°n CH√çNH X√ÅC 1 th·ªÉ lo·∫°i.
# - TUY·ªÜT ƒê·ªêI KH√îNG tr·∫£ v·ªÅ "Other" n·∫øu c√≤n b·∫•t k·ª≥ c√°ch suy ƒëo√°n h·ª£p l√Ω n√†o.
# - "Other" CH·ªà ƒë∆∞·ª£c d√πng khi keyword ho√†n to√†n v√¥ nghƒ©a, spam, ho·∫∑c kh√¥ng li√™n quan n·ªôi dung gi·∫£i tr√≠.

# NHI·ªÜM V·ª§:
# 1. Chu·∫©n ho√° v√† s·ª≠a l·ªói keyword (CH·ªà ƒë·ªÉ suy lu·∫≠n n·ªôi b·ªô, KH√îNG ghi ra output).
# 2. Nh·∫≠n di·ªán √Ω nghƒ©a g·ªëc g·∫ßn ƒë√∫ng nh·∫•t (phim, show, b√†i h√°t, s·ª± ki·ªán, ƒë·ªôi tuy·ªÉn, m√¥ t·∫£ n·ªôi dung).
# 3. G√°n th·ªÉ lo·∫°i PH√ô H·ª¢P NH·∫§T trong danh s√°ch d∆∞·ªõi ƒë√¢y.

# DANH S√ÅCH TH·ªÇ LO·∫†I H·ª¢P L·ªÜ (CH·ªà ƒê∆Ø·ª¢C CH·ªåN 1):
# - Action
# - Romance
# - Comedy
# - Horror
# - Animation
# - Drama
# - C Drama
# - K Drama
# - Sports
# - Music
# - Reality Show
# - TV Channel
# - News
# - Other

# LU·∫¨T SUY DI·ªÑN ∆ØU TI√äN (PH·∫¢I TU√ÇN THEO):
# - C√≥ "t·∫≠p", "episode", "ep":
#     ‚Ä¢ N·∫øu keyword ch·ª©a t√™n show / gameshow / reality quen thu·ªôc
#       (v√≠ d·ª•: running man, 2 ng√†y 1 ƒë√™m, rap vi·ªát, the voice, masterchef)
#       ‚Üí Reality Show
#     ‚Ä¢ N·∫øu ch·ª©a t·ª´ kho√° phim / series / h√†nh ƒë·ªông
#       ‚Üí Drama ho·∫∑c Action
# - Karaoke, b√†i h√°t, ca sƒ©, l·ªùi b√†i h√°t, remix ‚Üí Music
# - Tr·∫≠n ƒë·∫•u, b√≥ng ƒë√°, ƒë·ªôi tuy·ªÉn, U19, Vi·ªát Nam vs ‚Üí Sports
# - T√™n phim, series, ti√™u ƒë·ªÅ truy·ªán (k·ªÉ c·∫£ m∆° h·ªì, vi·∫øt sai) ‚Üí Drama
# - Phim Trung Qu·ªëc ‚Üí C Drama
# - Phim H√†n Qu·ªëc ‚Üí K Drama
# - Show truy·ªÅn h√¨nh, gameshow ‚Üí Reality Show
# - T√™n k√™nh (VTV, HTV, K+, HBO, Channel) ‚Üí TV Channel
# - Keyword ng·∫Øn gi·ªëng t√™n ri√™ng / ti√™u ƒë·ªÅ ‚Üí ∆∞u ti√™n Drama ho·∫∑c Music
# - Ch·ªâ d√πng Other khi keyword kh√¥ng th·ªÉ g√°n v√†o b·∫•t k·ª≥ nh√≥m n√†o ·ªü tr√™n

# OUTPUT:
# - Ch·ªâ tr·∫£ v·ªÅ 1 JSON object
# - Key = keyword g·ªëc trong danh s√°ch (KH√îNG s·ª≠a)
# - Value = th·ªÉ lo·∫°i ƒë√£ ph√¢n lo·∫°i
# - KH√îNG gi·∫£i th√≠ch, KH√îNG th√™m text ngo√†i JSON

# Danh s√°ch keyword:
# {movie_list}
#     """

#     try:
#         resp = client.chat.completions.create(
#             model="gpt-5-nano",
#             messages=[{"role": "user", "content": prompt}],
#             response_format={"type": "json_object"}  
#         )

#         text = resp.choices[0].message.content
#         text = clean_llm_json(text)

#         parsed = json.loads(text)

#         # ƒê·∫£m b·∫£o ƒë·ªß key cho to√†n batch
#         return {k: parsed.get(k, "Other") for k in movie_list}

#     except Exception as e:
#         print("LLM JSON parse error:", e)
#         return {k: "Other" for k in movie_list}


async def api_worker(i, batch, semaphore, queue):
    async with semaphore:
        mapping = await classify_batch_async(batch)

        for k, v in mapping.items():
            await queue.put(
                json.dumps(
                    {"keyword": k, "category": v},
                    ensure_ascii=False
                ) + "\n"
            )

        if (i+1) % 5 == 0:
                print(f"Processed {(i + 1) * 500} keywords")


async def writer_worker(queue, path, buffer_size=100):
    buffer = []
    with open(path, "a", encoding="utf-8") as f:
        while True:
            line = await queue.get()
            if line is None:
                if buffer:
                    f.write("".join(buffer))
                    f.flush()
                    os.fsync(f.fileno())
                queue.task_done()
                break

            buffer.append(line)
            if len(buffer) >= buffer_size:
                f.write("".join(buffer))
                f.flush()
                os.fsync(f.fileno())
                buffer = []

            queue.task_done()




# def save_mapping(data, path):
#     """
#     Docstring for save_mapping
    
#     :param data: Description
#     :param path: Description
#     """
#     with open(path, "a", encoding="utf-8") as f:
#         for k, v in data.items():
#             f.write(
#                 json.dumps(
#                     {"keyword": k, "category": v},
#                     ensure_ascii=False
#                 ) + "\n"
#             )
#         f.flush()
#         os.fsync(f.fileno())

# ----------------------
# join 
# ----------------------


def join_category(data: pl.DataFrame, mapping_df: pl.DataFrame) -> pl.DataFrame:
    """
    Gh√©p th√¥ng tin category v√†o d·ªØ li·ªáu g·ªëc theo c·ªôt keyword.

    Parameters
    ----------
    data : polars.DataFrame
        DataFrame d·ªØ li·ªáu g·ªëc, b·∫Øt bu·ªôc c√≥ c·ªôt `keyword`.
    mapping_df : polars.DataFrame
        DataFrame mapping keyword‚Äìcategory, g·ªìm c√°c c·ªôt:
        - keyword
        - category

    Returns
    -------
    polars.DataFrame
        DataFrame ƒë·∫ßu ra sau khi ƒë∆∞·ª£c b·ªï sung c·ªôt `category`
    """
    data = data.join(
        mapping_df,
        on="keyword",
        how="left"
    )
    return data



# --------------------------
# control_flow
# --------------------------

async def control_flow_async():
    data = read_data("parquet", folder_path)
    keywords = get_data(data)
    init_output_folder(save_path)

    mapping_path = save_path + "mapping.jsonl"

    if os.path.exists(mapping_path):
        mapping_df = read_jsonl(mapping_path)
        classified = set(mapping_df["keyword"].to_list())
        keywords = [k for k in keywords if k not in classified]

    batches = list(chunks(keywords, 500))

    queue = asyncio.Queue(maxsize=2000)
    semaphore = asyncio.Semaphore(8)   # üî• N API WORKERS

    writer = asyncio.create_task(
        writer_worker(queue, mapping_path)
    )

    api_tasks = [
        asyncio.create_task(api_worker(i, batch, semaphore, queue))
        for i, batch in enumerate(batches)
    ]

    try:
        await asyncio.gather(*api_tasks, return_exceptions=True)
        
    except KeyboardInterrupt:
        print("‚èπ Interrupted safely")

    await queue.join()      # üîí ch·ªù ghi xong to√†n b·ªô
    await queue.put(None)   # k·∫øt th√∫c writer
    await writer

    # FINAL STEP (GI·ªÆ NGUY√äN)
    mapping_df = read_jsonl(mapping_path).unique(
        subset=["keyword"], keep="last"
    )

    final_df = join_category(data, mapping_df)
    save_data(final_df, save_path + "final.parquet")




# def control_flow():
#     """
#     Docstring for control_flow
#     """
#     # 1. read, transform and init folder
#     data = read_data("parquet", folder_path)
#     keywords = get_data(data)
#     init_output_folder(save_path)

#     mapping_path = save_path + "mapping.jsonl"

#     if os.path.exists(mapping_path):
#         mapping_df = read_data("jsonl", mapping_path)
#         classified = set(mapping_df["keyword"].to_list())
#         keywords = [k for k in keywords if k not in classified]


#     for batch in chunks(keywords, 500):
        
#         # 2.1 Call LLM for this batch
#         mapping = classify_batch(batch)

#         # 2.2 Save mapping immediately (checkpoint)
#         save_mapping(mapping, save_path + "mapping.jsonl")

#         # Logging
#         time.sleep(0.2)
#         print(f"Processed {len(mapping)} keywords")

#     # 3. Load mapping, deduplicate, join, write output
#     mapping_df = read_jsonl(save_path + "mapping.jsonl")

#     mapping_df = (
#         mapping_df
#         .unique(subset=["keyword"], keep="last")
#     )

#     final_df = join_category(data, mapping_df)
#     save_data(final_df, save_path + "final.parquet")


# def control_flow_test():
#     """
#     Test run: ch·ªâ ch·∫°y 1 batch ƒë·∫ßu ti√™n (10 keywords)
#     """

#     # 1. Read data
#     data = read_data("parquet", folder_path)
#     keywords = get_data(data)

#     print(f"Total unique keywords: {len(keywords)}")

#     # üîπ ch·ªâ l·∫•y 10 keyword ƒë·∫ßu
#     test_keywords = keywords[:10]
#     print("Test keywords:", test_keywords)

#     # 2. Call LLM
#     mapping = classify_batch(test_keywords)

#     print("LLM output mapping:")
#     print(mapping)

#     # 3. Save th·ª≠ ra file test
#     test_path = save_path + "mapping_test.jsonl"
#     os.makedirs(save_path, exist_ok=True)

#     save_mapping(mapping, test_path)

#     print(f"‚úÖ Saved test mapping to {test_path}")



if __name__ == "__main__": 
    start = time.time()
    
    asyncio.run(control_flow_async())
    
    end = time.time()
    print(f"Total time: {(end - start)/60:.2f} minutes")