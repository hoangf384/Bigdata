# Big Data Pipeline â€“ OTT Content Analytics

> **Má»¥c tiÃªu:** XÃ¢y dá»±ng end-to-end Big Data pipeline xá»­ lÃ½ log xem ná»™i dung vÃ  log tÃ¬m kiáº¿m cá»§a má»™t há»‡ thá»‘ng truyá»n hÃ¬nh OTT, cháº¡y hoÃ n toÃ n trÃªn mÃ¡y cÃ¡ nhÃ¢n (khÃ´ng dÃ¹ng cloud).

---

## Má»¥c lá»¥c

1. [Tá»•ng quan](#1-tá»•ng-quan)
2. [Kiáº¿n trÃºc há»‡ thá»‘ng (UML)](#2-kiáº¿n-trÃºc-há»‡-thá»‘ng-uml)
3. [Tech Stack](#3-tech-stack)
4. [Pipeline chi tiáº¿t](#4-pipeline-chi-tiáº¿t)
   - [Pipeline 1 â€“ log_content ETL](#pipeline-1--log_content-etl)
   - [Pipeline 2 â€“ log_search ETL + LLM Enrichment](#pipeline-2--log_search-etl--llm-enrichment)
5. [Cáº¥u trÃºc thÆ° má»¥c](#5-cáº¥u-trÃºc-thÆ°-má»¥c)
6. [ÄÃ£ lÃ m Ä‘Æ°á»£c](#6-Ä‘Ã£-lÃ m-Ä‘Æ°á»£c)
7. [Káº¿ hoáº¡ch tiáº¿p theo](#7-káº¿-hoáº¡ch-tiáº¿p-theo)
8. [Key Learnings](#8-key-learnings)
9. [HÆ°á»›ng dáº«n cháº¡y](#9-hÆ°á»›ng-dáº«n-cháº¡y)

---

## 1. Tá»•ng quan

Project mÃ´ phá»ng há»‡ thá»‘ng phÃ¢n tÃ­ch khÃ¡ch hÃ ng cá»§a má»™t ná»n táº£ng OTT (Over-The-Top) truyá»n hÃ¬nh Viá»‡t Nam. Dá»¯ liá»‡u bao gá»“m:

- **log_content**: Log hÃ nh vi xem ná»™i dung (phim, thá»ƒ thao, thiáº¿u nhi...) cá»§a tá»«ng khÃ¡ch hÃ ng, dáº¡ng JSON theo ngÃ y
- **log_search**: Log tá»« khoÃ¡ tÃ¬m kiáº¿m cá»§a ngÆ°á»i dÃ¹ng, dáº¡ng Parquet theo ngÃ y

Tá»« raw log, há»‡ thá»‘ng tá»•ng há»£p ra **customer profile 30 ngÃ y**: má»—i khÃ¡ch hÃ ng xem gÃ¬ nhiá»u nháº¥t, tÃ¬m kiáº¿m gÃ¬, vÃ  sá»Ÿ thÃ­ch cÃ³ thay Ä‘á»•i theo thÃ¡ng khÃ´ng.

---

## 2. Kiáº¿n trÃºc há»‡ thá»‘ng (UML)

```mermaid
flowchart TB
    subgraph Sources["Data Ingestion"]
        S1["log_content/*.json<br/>(daily, ~48M rows)"]
        S2["log_search/*.parquet<br/>(daily)"]
    end

    subgraph Processing["Data Transformation - Python + Spark"]
        subgraph P1["Pipeline 1"]
            C1["ETL 30 days"]
        end
        subgraph P2["Pipeline 2"]
            E1["ETL log search"]
            E2["LLM Enrichment"]
            E3["Category Mapping"]
            E4["Post Enrich"]
            E1 --> E4
            E2 --> E3 --> E4
        end
    end

    OpenAI(["OpenAI API"])

    subgraph DB["Loading & Visualization"]
        M1[("MySQL<br/>customer_content_stats")]
        M2[("MySQL<br/>customer_search_stats")]
        V1["Metabase"]
        V2["DuckDB"]
        M1 --> V1
        M2 --> V1
        M2 --> V2
    end

    S1 --> C1 --> M1
    S2 --> E1
    S2 --> E2
    E2 -- "API Call" --> OpenAI
    OpenAI -- "Response" --> E2
    E4 --> M2
```

---

## 3. Tech Stack

| NhÃ³m | CÃ´ng cá»¥ | Má»¥c Ä‘Ã­ch |
|---|---|---|
| **Core Processing** | Apache Spark 4.x (PySpark) | ETL, distributed processing |
| **LLM Enrichment** | OpenAI API (async) | PhÃ¢n loáº¡i keyword tÃ¬m kiáº¿m |
| **Storage** | MySQL (LTS) | Serving layer, OLAP |
| **BI & Visualization** | Metabase | Dashboard, ad-hoc query |
| **Prototyping** | Jupyter Notebook, DuckDB | EDA, light-weight transform |
| **Infrastructure** | Docker, Docker Compose | Orchestrate all services |

---

## 4. Pipeline chi tiáº¿t

### Pipeline 1 â€“ log_content ETL

**File:** `pipelines/log_content/etl_30_days.py`  
**Input:** `data/raw/log_content/*.json` (1 file/ngÃ y, ~48M rows/30 ngÃ y)  
**Output:** MySQL table `customer_content_stats`

```mermaid
flowchart LR
    A["raw JSON\n(daily)"] --> B["flatten _source.*"]
    B --> C["map AppName to Type<br/>5 categories"]
    C --> D["filter invalid\nContract"]
    D --> E["pivot\n1 row/Contract/day"]
    E --> F["union 30 days"]
    F --> G["find_active\ndistinct dates"]
    G --> H["most_watch\ngreatest duration"]
    H --> I["customer_taste\ntypes used"]
    I --> J[("MySQL\ncustomer_content_stats")]
```

**Schema output (1 row / Contract):**

| Cá»™t | MÃ´ táº£ |
|---|---|
| `Contract` | MÃ£ khÃ¡ch hÃ ng |
| `Truyen Hinh` | Tá»•ng duration xem truyá»n hÃ¬nh (30 ngÃ y) |
| `Phim Truyen` | Tá»•ng duration xem phim |
| `Giai Tri` | Tá»•ng duration ná»™i dung giáº£i trÃ­ |
| `Thieu Nhi` | Tá»•ng duration ná»™i dung thiáº¿u nhi |
| `The Thao` | Tá»•ng duration thá»ƒ thao |
| `Active` | Sá»‘ ngÃ y distinct cÃ³ hoáº¡t Ä‘á»™ng |
| `MostWatch` | Thá»ƒ loáº¡i xem nhiá»u nháº¥t |
| `Taste` | Combo thá»ƒ loáº¡i Ä‘Ã£ dÃ¹ng (vd: `Truyen Hinh-Phim Truyen`) |

---

### Pipeline 2 â€“ log_search ETL + LLM Enrichment

**Files:** `pipelines/log_search/` (4 scripts)  
**Input:** `data/raw/log_search/*.parquet` + OpenAI API  
**Output:** MySQL table `customer_search_stats`

```mermaid
flowchart TD
    A["raw parquet(daily)"] --> B["datetime_transform\nnormalize date"]
    B --> C["union by month"]
    C --> D["most_watch\ntop keyword/user"]
    D --> E["CSV: month=6\nmonth=7"]

    K["raw keywords"] --> L["enrich_v1.py\nasync OpenAI\nbatch 500 - 8 workers"]
    L --> M["mapping.jsonl\nkeyword - category"]
    M --> N["mapping.py\ncategory_std"]

    E --> O["join keyword\nto category"]
    N --> O
    O --> P["join June + July"]
    P --> Q["customertaste\nUnchanged / Changed"]
    Q --> R[("MySQL\ncustomer_search_stats")]
```

**Äiá»ƒm ká»¹ thuáº­t ná»•i báº­t:**
- `enrich_v1.py` dÃ¹ng **asyncio + Semaphore(8)** Ä‘á»ƒ gá»i OpenAI song song, trÃ¡nh rate limit
- **Producerâ€“Consumer pattern**: 8 API workers â†’ Queue â†’ 1 writer_worker (buffered I/O)
- **Checkpoint/resume**: skip keyword Ä‘Ã£ classified, trÃ¡nh gá»i API láº¡i khi restart
- **Prompt Engineering**: rule-based prompt tiáº¿ng Viá»‡t, 13 categories, Æ°u tiÃªn suy luáº­n trÆ°á»›c khi fallback `Other`

---

## 5. Cáº¥u trÃºc thÆ° má»¥c

```
Bigdata/
â”œâ”€â”€ pipelines/
â”‚   â”œâ”€â”€ log_content/
â”‚   â”‚   â””â”€â”€ etl_30_days.py          â† Pipeline 1
â”‚   â””â”€â”€ log_search/
â”‚       â”œâ”€â”€ etl_log_search.py       â† Pipeline 2a: ETL theo thÃ¡ng
â”‚       â”œâ”€â”€ enrich_v1.py            â† Pipeline 2b: LLM enrichment
â”‚       â”œâ”€â”€ mapping.py              â† Pipeline 2c: Spark category std
â”‚       â””â”€â”€ post_enrich.py          â† Pipeline 2d: join + load MySQL
â”‚
â”œâ”€â”€ notebooks/                      â† EDA & prototyping (6 notebooks)
â”‚   â”œâ”€â”€ class 5.ipynb               â† ETL experiments
â”‚   â”œâ”€â”€ log_search_v1.ipynb         â† log search EDA
â”‚   â””â”€â”€ enrich.ipynb                â† LLM enrichment test
â”‚
â”œâ”€â”€ queries/queries/                â† SQL analysis queries
â”‚
â”œâ”€â”€ infra/
â”‚   â”œâ”€â”€ spark/
â”‚   â”‚   â”œâ”€â”€ docker-compose.yaml     â† 1 master + 2 workers
â”‚   â”‚   â”œâ”€â”€ Dockerfile              â† custom image + JDBC jars
â”‚   â”‚   â””â”€â”€ jars/                   â† MySQL connector JAR
â”‚   â”œâ”€â”€ mysql/
â”‚   â”‚   â”œâ”€â”€ docker-compose.yaml
â”‚   â”‚   â””â”€â”€ init/                   â† init SQL scripts
â”‚   â”œâ”€â”€ metabase/
â”‚   â”‚   â””â”€â”€ docker-compose.yaml     â† BI dashboard
â”‚   â””â”€â”€ jupyter/
â”‚       â””â”€â”€ docker-compose.yaml
â”‚
â”œâ”€â”€ docs/                           â† TÃ i liá»‡u ká»¹ thuáº­t
â”œâ”€â”€ .env                            â† API keys, DB credentials
â”œâ”€â”€ .gitignore
â””â”€â”€ README.md
```

---

## 6. ÄÃ£ lÃ m Ä‘Æ°á»£c

### âœ… Infrastructure
- [x] Dá»±ng Spark cluster local báº±ng Docker (1 master + 2 workers)
- [x] Cáº¥u hÃ¬nh MySQL vá»›i healthcheck, utf8mb4, connection pool
- [x] Setup Metabase káº¿t ná»‘i MySQL Ä‘á»ƒ visualize
- [x] Cáº¥u hÃ¬nh Jupyter Notebook vá»›i Spark kernel

### âœ… Pipeline 1 â€“ log_content
- [x] Äá»c vÃ  xá»­ lÃ½ 48M rows JSON â†’ 1.9M rows sau aggregate
- [x] Transform: flatten nested JSON, map AppName â†’ category
- [x] TÃ­nh `Active`, `MostWatch`, `Taste` cho má»—i khÃ¡ch hÃ ng
- [x] Load vÃ o MySQL qua JDBC (batch insert 10K rows)
- [x] Refactor qua 3 versions, version cuá»‘i cÃ³ docstring Ä‘áº§y Ä‘á»§

### âœ… Pipeline 2 â€“ log_search
- [x] ETL log search theo ngÃ y â†’ tá»•ng há»£p theo thÃ¡ng
- [x] TÃ¬m keyword Ä‘Æ°á»£c search nhiá»u nháº¥t cá»§a tá»«ng user (`mostWatch`)
- [x] LLM enrichment vá»›i OpenAI: phÃ¢n loáº¡i ~unique keywords thÃ nh 13 categories
- [x] Async pipeline vá»›i checkpoint/resume (khÃ´ng gá»i API láº¡i)
- [x] Join 2 thÃ¡ng, táº¡o cá»™t `taste` (Unchanged/Changed)
- [x] Load káº¿t quáº£ vÃ o MySQL

### âœ… Project Organization
- [x] TÃ¡i cáº¥u trÃºc project tá»« flat â†’ pipeline-based layout
- [x] XÃ³a deprecated ETL versions (v1, v2)
- [x] Gitignore logs, data, venv
- [x] Viáº¿t README Ä‘áº§y Ä‘á»§ vá»›i Mermaid diagrams

---

## 7. Káº¿ hoáº¡ch tiáº¿p theo

### ğŸ”² Ngáº¯n háº¡n
- [ ] Viáº¿t SQL queries phÃ¢n tÃ­ch: top content by region, churn prediction features
- [ ] HoÃ n thiá»‡n Metabase dashboard: 3â€“5 charts tá»« `customer_content_stats`
- [ ] Update volume paths trong `infra/spark/docker-compose.yaml` (hiá»‡n trá» path cÅ©)
- [ ] Viáº¿t `init/` SQL schema cho MySQL (hiá»‡n Ä‘ang rá»—ng)

### ğŸ”² Trung háº¡n
- [ ] ThÃªm **Kafka** Ä‘á»ƒ simulate real-time log ingestion
- [ ] Viáº¿t **Airflow DAG** orchestrate Pipeline 2 (4 bÆ°á»›c phá»¥ thuá»™c nhau)
- [ ] ThÃªm **dbt** Ä‘á»ƒ quáº£n lÃ½ SQL transformation layer
- [ ] ThÃªm **data quality checks** (Great Expectations hoáº·c tá»± viáº¿t)

### ğŸ”² DÃ i háº¡n
- [ ] Deploy lÃªn cloud (AWS EMR hoáº·c GCP Dataproc)
- [ ] ThÃªm **Cassandra** lÃ m hot storage cho real-time query
- [ ] Viáº¿t Scala version cá»§a ETL Ä‘á»ƒ so sÃ¡nh performance vá»›i PySpark

---

## 8. Key Learnings

### Apache Spark
- Hiá»ƒu **lazy evaluation**: transformation chá»‰ thá»±c thi khi cÃ³ action
- `spark.sql.shuffle.partitions` áº£nh hÆ°á»Ÿng lá»›n Ä‘áº¿n performance vá»›i groupBy/join
- `repartition()` trÆ°á»›c khi write giÃºp kiá»ƒm soÃ¡t sá»‘ file output
- `unionByName()` an toÃ n hÆ¡n `union()` khi schema cÃ³ thá»ƒ khÃ¡c thá»© tá»± cá»™t
- JDBC write vá»›i `batchsize=10000` tÄƒng tá»‘c Ä‘Ã¡ng ká»ƒ so vá»›i máº·c Ä‘á»‹nh

### Python Async
- `asyncio.Semaphore` Ä‘á»ƒ giá»›i háº¡n concurrent API calls, trÃ¡nh rate limit
- **Producerâ€“Consumer pattern** vá»›i `asyncio.Queue`: tÃ¡ch logic gá»i API vÃ  ghi file
- Buffered I/O + `os.fsync()` Ä‘áº£m báº£o dá»¯ liá»‡u khÃ´ng máº¥t khi crash
- Checkpoint báº±ng cÃ¡ch Ä‘á»c file Ä‘Ã£ ghi â†’ skip nhá»¯ng gÃ¬ Ä‘Ã£ xá»­ lÃ½ (resume-safe)

### Docker & Infrastructure
- Hiá»ƒu volume mount giá»¯a host vÃ  container (`:z` flag cho SELinux trÃªn Fedora)
- External network `spark-net` Ä‘á»ƒ cÃ¡c compose file khÃ¡c nhau giao tiáº¿p
- JDBC JAR pháº£i Ä‘Æ°á»£c mount vÃ o Ä‘Ãºng classpath cá»§a Spark executor

### Data Engineering Patterns
- **Grain**: luÃ´n xÃ¡c Ä‘á»‹nh rÃµ grain cá»§a DataFrame trÆ°á»›c khi transform
- **Pivot table**: chuyá»ƒn long format â†’ wide format cho customer-level analytics
- **Window function**: `row_number().over(Window.partitionBy())` Ä‘á»ƒ láº¥y top-1 per group
- **LLM as enrichment layer**: dÃ¹ng LLM Ä‘á»ƒ label unstructured data (keyword â†’ category)

---

## 9. HÆ°á»›ng dáº«n cháº¡y

### Khá»Ÿi Ä‘á»™ng services

```bash
# Spark cluster
docker compose -f infra/spark/docker-compose.yaml up -d

# MySQL
docker compose -f infra/mysql/docker-compose.yaml up -d

# Metabase (optional)
docker compose -f infra/metabase/docker-compose.yaml up -d
```

### Cháº¡y Pipeline 1 â€“ log_content

```bash
docker exec spark-master /opt/spark/bin/spark-submit \
  --master spark://spark-master:7077 \
  --jars /opt/spark/jars_external/mysql-connector-j-8.4.0.jar \
  /code/pipelines/log_content/etl_30_days.py
```

### Cháº¡y Pipeline 2 â€“ log_search

```bash
# BÆ°á»›c 1: ETL log search
docker exec spark-master /opt/spark/bin/spark-submit \
  /code/pipelines/log_search/etl_log_search.py

# BÆ°á»›c 2: LLM enrichment (local, cáº§n .env vá»›i OPENAI_API_KEY)
source .venv/bin/activate
python pipelines/log_search/enrich_v1.py

# BÆ°á»›c 3: Spark category mapping
docker exec spark-master /opt/spark/bin/spark-submit \
  /code/pipelines/log_search/mapping.py

# BÆ°á»›c 4: Post enrich + load MySQL
docker exec spark-master /opt/spark/bin/spark-submit \
  --jars /opt/spark/jars_external/mysql-connector-j-8.4.0.jar \
  /code/pipelines/log_search/post_enrich.py
```

### DuckDB ad-hoc query

```bash
duckdb ~/Bigdata/DuckDB/db/mydatabase.duckdb -ui
```

---

*Project Ä‘ang trong giai Ä‘oáº¡n há»c táº­p vÃ  phÃ¡t triá»ƒn liÃªn tá»¥c.*
